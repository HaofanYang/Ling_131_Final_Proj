{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Running classifiers </h1>\n",
    "\n",
    "***\n",
    "Six classifiers were trained and tested. To enable running these trained classifiers on another email dataset, they were saved after being trained so that they can be loaded promptly later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving classifiers\n",
    "\n",
    "The **joblib** module in **sklearn.externals** comes handy for this purpose. Suppose we have a trained classifier **clf** and in order to save it, simply run **joblib.dump(clf, filename)**. The saving functionality was embeded within the method test_clf and after traning, six **joblib** files will be saved\n",
    "\n",
    "> 1. MultinomialNB.joblib\n",
    "> 2. RandomForest.joblib\n",
    "> 3. Ridge_Regression.joblib\n",
    "> 4. BernoulliNB.joblib\n",
    "> 5. K_Neighbors.joblib\n",
    "> 6. SVC.joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(name, clf):\n",
    "    print (u'Classifier：', clf)\n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)\n",
    "    print (u'Training time for 5 -fold cross validation：%.3f/(5*%d) = %.3fsec' % ((t_end - t_start), m, t_train))\n",
    "    print( u'Optimal hyperparameter：', model.best_params_)\n",
    "    joblib.dump(model, \"%s.joblib\"%name)\n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test)\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start\n",
    "    print (u'Testing Time：%.3f sec' % t_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    print (u'Accuracy ：%.2f%%' % (100 * acc))\n",
    "    name = str(clf).split('(')[0]\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]     # 去掉末尾的Classifier\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "    return t_train, t_test, 1-acc, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving vectorizer \n",
    "\n",
    "In order to run classifiers on another dataset, the new dataset has to be preprocessed in exactly the same way of treating the original dataset. This requires to save the **TfidfVectorizer** after fitting it with **data_train**. New raw text will be transformed into **TF-IDF** vectors using this very vectorizer. In a similar way as saving the classifiers, the python in-built module **pickle** was utilized to save the **vectorizer**. The vectorizer was saved in the file ****vec.pickle****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(data_train, data_test):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)\n",
    "\n",
    "    vec = vectorizer.fit(data_train.data)\n",
    "    pickle.dump(vec, open(\"vec.pickle\", \"wb\"))\n",
    "    x_train = vectorizer.transform(data_train.data)  # x_train是稀疏的，scipy.sparse.csr.csr_matrix\n",
    "    x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "    return x_train, x_test, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
