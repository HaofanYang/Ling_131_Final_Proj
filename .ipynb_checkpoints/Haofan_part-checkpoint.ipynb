{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Running classifiers </h1>\n",
    "\n",
    "***\n",
    "Six classifiers were trained and tested. To enable running these trained classifiers on another email dataset, they were saved after being trained so that they can be loaded promptly later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Saving classifiers\n",
    "\n",
    "The **joblib** module in **sklearn.externals** comes handy for this purpose. Suppose we have a trained classifier **clf** and in order to save it, simply run **joblib.dump(clf, filename)**. The saving functionality was embeded within the method **test_clf** and after traning, six **joblib** files will be saved as\n",
    "\n",
    "> 1. MultinomialNB.joblib\n",
    "> 2. RandomForest.joblib\n",
    "> 3. Ridge_Regression.joblib\n",
    "> 4. BernoulliNB.joblib\n",
    "> 5. K_Neighbors.joblib\n",
    "> 6. SVC.joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(name, clf):\n",
    "    print (u'Classifier：', clf)\n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)\n",
    "    print (u'Training time for 5 -fold cross validation：%.3f/(5*%d) = %.3fsec' % ((t_end - t_start), m, t_train))\n",
    "    print( u'Optimal hyperparameter：', model.best_params_)\n",
    "    joblib.dump(model, \"%s.joblib\"%name)\n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test)\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start\n",
    "    print (u'Testing Time：%.3f sec' % t_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    print (u'Accuracy ：%.2f%%' % (100 * acc))\n",
    "    name = str(clf).split('(')[0]\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]     # 去掉末尾的Classifier\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "    return t_train, t_test, 1-acc, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving vectorizer \n",
    "\n",
    "In order to run classifiers on another dataset, the new dataset has to be preprocessed in exactly the same way of treating the original dataset. This requires to save the **TfidfVectorizer** after fitting it with **data_train**. New raw text will be transformed into **TF-IDF** vectors using this very vectorizer. In a similar way as saving the classifiers, the python in-built module **pickle** was utilized to save the **vectorizer**. The vectorizer was saved in the file ****vec.pickle****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(data_train, data_test):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)\n",
    "\n",
    "    vec = vectorizer.fit(data_train.data)\n",
    "    pickle.dump(vec, open(\"vec.pickle\", \"wb\"))\n",
    "    x_train = vectorizer.transform(data_train.data)  # x_train是稀疏的，scipy.sparse.csr.csr_matrix\n",
    "    x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "    return x_train, x_test, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running classifiers\n",
    "\n",
    "First specify the filename of the raw text and transform it into the **TF-IDF** vector using the method **get_tfidf**.\n",
    "\n",
    "### Loading the vectorizer\n",
    "\n",
    "The vectorized was fit and saved in the file **vec.pickle**. In order to load it, run **pickle.load(\"vec.pickle\")**. The raw text can be transformed into a **TF-IDF** vector using this vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 33654)\t0.058523288651348045\n",
      "  (0, 33597)\t0.07950599605058221\n",
      "  (0, 33381)\t0.03840068981835156\n",
      "  (0, 33366)\t0.05499176868335385\n",
      "  (0, 33342)\t0.02995778455336368\n",
      "  (0, 33334)\t0.0397846329216497\n",
      "  (0, 33309)\t0.01844672331708908\n",
      "  (0, 33303)\t0.029723119935018725\n",
      "  (0, 33288)\t0.02270583426947962\n",
      "  (0, 33182)\t0.0423064812093078\n",
      "  (0, 33062)\t0.08096440111054665\n",
      "  (0, 32993)\t0.07025388891618414\n",
      "  (0, 32928)\t0.09288239260230895\n",
      "  (0, 32849)\t0.03967538645022774\n",
      "  (0, 32547)\t0.04548999989759632\n",
      "  (0, 32504)\t0.04618701148205565\n",
      "  (0, 32429)\t0.07894749349356875\n",
      "  (0, 32420)\t0.028584813824891843\n",
      "  (0, 32336)\t0.038923275399569364\n",
      "  (0, 32232)\t0.021168942671621935\n",
      "  (0, 32207)\t0.04781887956354195\n",
      "  (0, 32152)\t0.04372636092767193\n",
      "  (0, 32141)\t0.051256056735957026\n",
      "  (0, 31967)\t0.020312897962306932\n",
      "  (0, 31746)\t0.024577697676100376\n",
      "  :\t:\n",
      "  (0, 6590)\t0.04322181009406479\n",
      "  (0, 6579)\t0.035351909389793566\n",
      "  (0, 6569)\t0.05560959863517859\n",
      "  (0, 6568)\t0.074594107888131\n",
      "  (0, 6496)\t0.04372636092767193\n",
      "  (0, 6384)\t0.031165027038861197\n",
      "  (0, 6058)\t0.033588270419869164\n",
      "  (0, 5757)\t0.028389786628514165\n",
      "  (0, 5702)\t0.012400122085389723\n",
      "  (0, 5471)\t0.05499176868335385\n",
      "  (0, 5429)\t0.05288792481744333\n",
      "  (0, 5302)\t0.028389786628514165\n",
      "  (0, 5199)\t0.04149307852430779\n",
      "  (0, 4989)\t0.03399092733031786\n",
      "  (0, 4987)\t0.03195640233838287\n",
      "  (0, 4774)\t0.026681341025856082\n",
      "  (0, 4515)\t0.07092335637080831\n",
      "  (0, 4513)\t0.06084129057773925\n",
      "  (0, 4438)\t0.04485367817555109\n",
      "  (0, 4347)\t0.06540330872042803\n",
      "  (0, 4312)\t0.03486955306927604\n",
      "  (0, 2316)\t0.03385423014566799\n",
      "  (0, 917)\t0.028014674332360642\n",
      "  (0, 814)\t0.051256056735957026\n",
      "  (0, 571)\t0.027834095121814718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "run_file = \"test.txt\" # Please specify the file name you want to test\n",
    "\n",
    "def get_tfidf(filename):\n",
    "\ttext = \"\"\n",
    "\tvectorizer = pickle.load(open(\"vec.pickle\"))\n",
    "\twith open(filename) as fl:\n",
    "\t\tfor line in fl:\n",
    "\t\t\ttext += line\n",
    "\treturn vectorizer.transform([text]) \n",
    "\n",
    "x = get_tfidf(run_file)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading classifiers\n",
    "\n",
    "The raw text has been transformed into the **TF-IDF** vector and it can be classified using saved classifiers. Define a method **run(x, clf_name)** where x is a **TF-IDF** vector and **clf_name** is the filename for a classifier. It returns the y value for the given x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x, clf_name):\n",
    "\tclf_name = clf_name + \".joblib\"\n",
    "\tclf = joblib.load(clf_name)\n",
    "\treturn clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification for test.txt is: \n",
      "\n",
      "MultinomialNB: talk.religion.misc\n",
      "\n",
      "BernoulliNB: talk.religion.misc\n",
      "\n",
      "K_Neighbors: talk.religion.misc\n",
      "\n",
      "Ridge_Regression: talk.religion.misc\n",
      "\n",
      "RandomForest: talk.religion.misc\n",
      "\n",
      "SVC: talk.religion.misc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanghaofan/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "print(\"The classification for %s is: \\n\" % run_file)\n",
    "clfs = (\"MultinomialNB\", \"BernoulliNB\",  \"K_Neighbors\",  \"Ridge_Regression\",  \"RandomForest\",  \"SVC\")\n",
    "cat = ('alt.atheism', 'comp.graphics','sci.space', 'talk.religion.misc')\n",
    "for clf in clfs:\n",
    "    print(\"%s: %s\\n\" % (clf, cat[run(x, clf)[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
