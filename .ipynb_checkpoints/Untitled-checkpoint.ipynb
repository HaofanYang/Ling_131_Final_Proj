{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation ##\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Training Classifiers ###\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Import ####\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "from sklearn.externals import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Get Dataset ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a built in method in **sklearn.dataset** to downloads data from 20newsgroups api with some chosen categories (computer science, science, electronics, sports), seperates training set and testing set, finally returns them.\n",
    "\n",
    "If these files already exisits locally, it will just load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    remove = ()\n",
    "    categories = 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'\n",
    "\n",
    "    print('start downloading...')\n",
    "    t_start = time()\n",
    "\n",
    "    data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "    data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "\n",
    "    t_end = time()\n",
    "    print('downloading completed，take %.3f sec' % (t_end - t_start))\n",
    "\n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method prints type, size, and categories of training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start downloading...\n",
      "downloading completed，take 0.517 sec\n",
      "data type： <class 'sklearn.utils.Bunch'>\n",
      "# of texts in train set ： 2034\n",
      "# of texts in test set： 1353\n",
      "name of4 categories：\n",
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(data_train, data_test):\n",
    "\n",
    "    print('data type：', type(data_train))\n",
    "    print('# of texts in train set ：', len(data_train.data))\n",
    "    print('# of texts in test set：', len(data_test.data))\n",
    "    print('name of%d categories：' % len(data_train.target_names))\n",
    "\n",
    "    pprint(data_train.target_names)\n",
    "\n",
    "data_train, data_test = get_data()\n",
    "print_data_info(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_y_data** method simply returns the label array of training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_data(data_train, data_test):\n",
    "\n",
    "    y_train = data_train.target\n",
    "    y_test = data_test.target\n",
    "\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**print_examples** prints 2 example data in training set with their corrisponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Examples : the first 2 texts -- \n",
      "----------------\n",
      "category for text1 : alt.atheism\n",
      "\n",
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: note to Bobby M.\n",
      "Lines: 52\n",
      "Organization: Walla Walla College\n",
      "Lines: 52\n",
      "\n",
      "In article <1993Apr14.190904.21222@daffy.cs.wisc.edu> mccullou@snake2.cs.wisc.edu (Mark McCullough) writes:\n",
      ">From: mccullou@snake2.cs.wisc.edu (Mark McCullough)\n",
      ">Subject: Re: note to Bobby M.\n",
      ">Date: Wed, 14 Apr 1993 19:09:04 GMT\n",
      ">In article <1993Apr14.131548.15938@monu6.cc.monash.edu.au> darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\n",
      ">>In <madhausC5CKIp.21H@netcom.com> madhaus@netcom.com (Maddi Hausmann) writes:\n",
      ">>\n",
      ">>>Mark, how much do you *REALLY* know about vegetarian diets?\n",
      ">>>The problem is not \"some\" B-vitamins, it's balancing proteins.  \n",
      ">>>There is also one vitamin that cannot be obtained from non-animal\n",
      ">>>products, and this is only of concern to VEGANS, who eat no\n",
      ">>>meat, dairy, or eggs.  I believe it is B12, and it is the only\n",
      ">>>problem.  Supplements are available for vegans; yes, the B12\n",
      ">>>does come from animal by-products.  If you are on an ovo-lacto\n",
      ">>>vegetarian diet (eat dairy and eggs) this is not an issue.\n",
      ">\n",
      ">I didn't see the original posting, but...\n",
      ">Yes, I do know about vegetarian diets, considering that several of my\n",
      ">close friends are devout vegetarians, and have to take vitamin supplements.\n",
      ">B12 was one of the ones I was thinking of, it has been a long time since\n",
      ">I read the article I once saw talking about the special dietary needs\n",
      ">of vegetarians so I didn't quote full numbers.  (Considering how nice\n",
      ">this place is. ;)\n",
      ">\n",
      ">>B12 can also come from whole-grain rice, I understand.  Some brands here\n",
      ">>in Australia (and other places too, I'm sure) get the B12 in the B12\n",
      ">>tablets from whole-grain rice.\n",
      ">\n",
      ">Are you sure those aren't an enriched type?  I know it is basically\n",
      ">rice and soybeans to get almost everything you need, but I hadn't heard\n",
      ">of any rice having B12.  \n",
      ">\n",
      ">>Just thought I'd contribute on a different issue from the norm :)\n",
      ">\n",
      ">You should have contributed to the programming thread earlier. :)\n",
      ">\n",
      ">> Fred Rice\n",
      ">> darice@yoyo.cc.monash.edu.au   \n",
      ">\n",
      ">M^2\n",
      ">\n",
      "If one is a vegan (a vegetarian taht eats no animal products at at i.e eggs, \n",
      "milk, cheese, etc., after about 3 years of a vegan diet, you need to start \n",
      "taking B12 supplements because b12 is found only in animals.) Acutally our \n",
      "bodies make B12, I think, but our bodies use up our own B12 after 2 or 3 \n",
      "years.  \n",
      "Lacto-oveo vegetarians, like myself, still get B12 through milk products \n",
      "and eggs, so we don't need supplements.\n",
      "And If anyone knows more, PLEASE post it.  I'm nearly contridicting myself \n",
      "with the mish-mash of knowledge I've gleaned.\n",
      "\n",
      "Tammy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "category for text2 : comp.graphics\n",
      "\n",
      "From: ch381@cleveland.Freenet.Edu (James K. Black)\n",
      "Subject: NEEDED: algorithms for 2-d & 3-d object recognition\n",
      "Organization: Case Western Reserve University, Cleveland, OH (USA)\n",
      "Lines: 23\n",
      "Reply-To: ch381@cleveland.Freenet.Edu (James K. Black)\n",
      "NNTP-Posting-Host: hela.ins.cwru.edu\n",
      "\n",
      "\n",
      "Hi,\n",
      "         I have a friend who is working on 2-d and 3-d object recognition. He is looking\n",
      "for references describing algorithms on the following subject areas:\n",
      "\n",
      "Thresholding\n",
      "Edge Segmentation\n",
      "Marr-Hildreth\n",
      "Sobel Operator\n",
      "Chain Codes\n",
      "Thinning - Skeletonising\n",
      "\n",
      "If anybody is willing to post an algorithm that they have implemented which demonstrates\n",
      "any of the above topics, it would be much appreciated.\n",
      "\n",
      "Please post all replies to my e-mail address. If requested I will post a summary to the\n",
      "newsgroup in a couple of weeks.\n",
      "\n",
      "\n",
      "Thanks in advance for all replies\n",
      "\n",
      "James\n",
      "eb192@city.ac.uk\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_examples(y_train, data_train):\n",
    "\n",
    "    print(' -- Examples : the first 2 texts -- ')\n",
    "\n",
    "    categories = data_train.target_names\n",
    "\n",
    "    for i in np.arange(2):\n",
    "        print('----------------')\n",
    "        print('category for text%d : %s\\n' % (i + 1, categories[y_train[i]]))\n",
    "        print(data_train.data[i])\n",
    "        print('\\n\\n')\n",
    "\n",
    "y_train, y_test = get_y_data(data_train, data_test)\n",
    "print_examples(y_train, data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3) Fit Data using TF-IDF Model ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call method **tfidf_data** to fit training set and testing set using **TF-IDF**, meanwhile save the trained data to **vec.pickle**, avoiding training classifiers every time we run the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(data_train, data_test):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)\n",
    "\n",
    "    vec = vectorizer.fit(data_train.data)\n",
    "    pickle.dump(vec, open(\"vec.pickle\", \"wb\"))\n",
    "    x_train = vectorizer.transform(data_train.data)  # x_train is sparse，scipy.sparse.csr.csr_matrix\n",
    "    x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "    return x_train, x_test, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It prints size and number of features of training set after **TF-IDF** fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train set：2034，# of features：33809\n"
     ]
    }
   ],
   "source": [
    "def print_x_data(x_train, vectorizer):\n",
    "\n",
    "    print('# of train set：%d，# of features：%d' % x_train.shape)\n",
    "    \n",
    "x_train, x_test, vectorizer = tfidf_data(data_train, data_test)\n",
    "print_x_data(x_train, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4) Train Classifier ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method **classifier** uses Multinomial Naive Bayes classifier, Bernoulli Naive Bayes classifier, K Neighbors classifier, Ridge Regression classifier, Random Forest classifier, Support Vector Machine classifier to classify training data. \n",
    "\n",
    "It then returns array of results of each classifier including its error rate, training time and testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(x, y):\n",
    "    print('\\n\\n===================\\n evaluation of classifiers：\\n')\n",
    "    clfs = {\"MultinomialNB\": MultinomialNB(), \n",
    "            \"BernoulliNB\": BernoulliNB(),  \n",
    "            \"K_Neighbors\": KNeighborsClassifier(),  \n",
    "            \"Ridge_Regression\": RidgeClassifier(),  \n",
    "            \"RandomForest\": RandomForestClassifier(n_estimators=200),  \n",
    "            \"SVC\": SVC()  \n",
    "            }\n",
    "    result = []\n",
    "    for name,clf in clfs.items():\n",
    "        a = test_clf(name, clf, x, y)\n",
    "        result.append(a)\n",
    "        print('\\n')\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test_clf**'s input is one of the classifiers. GridSearchCV object's job is to set parameters to the classifier according to type of the classifier (e.g. set **neighbors_can** when the classifier is **K Neighbors classifier**). \n",
    "\n",
    "Then it trains each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================\n",
      " evaluation of classifiers：\n",
      "\n",
      "Classifier： MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Training time for 5 -fold cross validation：0.375/(5*10) = 0.008sec\n",
      "Optimal hyperparameter： {'alpha': 0.003593813663804626}\n",
      "Testing Time：0.001 sec\n",
      "Accuracy ：89.58%\n",
      "\n",
      "\n",
      "Classifier： BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Training time for 5 -fold cross validation：0.523/(5*10) = 0.010sec\n",
      "Optimal hyperparameter： {'alpha': 0.001}\n",
      "Testing Time：0.002 sec\n",
      "Accuracy ：88.54%\n",
      "\n",
      "\n",
      "Classifier： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Training time for 5 -fold cross validation：11.131/(5*14) = 0.159sec\n",
      "Optimal hyperparameter： {'n_neighbors': 3}\n",
      "Testing Time：0.137 sec\n",
      "Accuracy ：86.03%\n",
      "\n",
      "\n",
      "Classifier： RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "Training time for 5 -fold cross validation：17.442/(5*10) = 0.349sec\n",
      "Optimal hyperparameter： {'alpha': 0.01291549665014884}\n",
      "Testing Time：0.002 sec\n",
      "Accuracy ：89.43%\n",
      "\n",
      "\n",
      "Classifier： RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Training time for 5 -fold cross validation：23.508/(5*6) = 0.784sec\n",
      "Optimal hyperparameter： {'max_depth': 9}\n",
      "Testing Time：0.114 sec\n",
      "Accuracy ：77.09%\n",
      "\n",
      "\n",
      "Classifier： SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training time for 5 -fold cross validation：208.636/(5*9) = 4.636sec\n",
      "Optimal hyperparameter： {'C': 100.0, 'gamma': 0.03162277660168379}\n",
      "Testing Time：1.413 sec\n",
      "Accuracy ：90.10%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_clf(name, clf, x_train, y_train):\n",
    "    print ('Classifier：', clf)\n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)\n",
    "    print ('Training time for 5 -fold cross validation：%.3f/(5*%d) = %.3fsec' % ((t_end - t_start), m, t_train))\n",
    "    print( 'Optimal hyperparameter：', model.best_params_)\n",
    "    joblib.dump(model, \"%s.joblib\"%name)\n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test)\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start\n",
    "    print ('Testing Time：%.3f sec' % t_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    print ('Accuracy ：%.2f%%' % (100 * acc))\n",
    "    name = str(clf).split('(')[0]\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "    return t_train, t_test, 1-acc, name\n",
    "\n",
    "\n",
    "\n",
    "result = classifier(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5) Render Results ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**draw** renders a diagram using results from previous method to evaluate results of different classifiers by listing the **error rate, training time and testing time** of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(result):\n",
    "    time_train1, time_test1, err1, names = result.T\n",
    "    time_test = time_test1.astype(np.float)\n",
    "    time_train = time_train1.astype(np.float)\n",
    "    err = err1.astype(np.float)\n",
    "    x= np.arange(len(time_train))\n",
    "    bar_width = 0.25\n",
    "    ax1 = host_subplot(111, axes_class=AA.Axes)\n",
    "    plt.subplots_adjust(right = 0.75)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    offset3 = 60\n",
    "    offset2 = 0\n",
    "\n",
    "    new_fixed_axis = ax3.get_grid_helper().new_fixed_axis\n",
    "    ax3.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=ax3, offset=(offset3, 0))\n",
    "    ax3.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    new_fixed_axis2 = ax2.get_grid_helper().new_fixed_axis\n",
    "    ax2.axis[\"right\"] = new_fixed_axis2(loc=\"right\", axes=ax2, offset=(offset2, 0))\n",
    "    ax2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    ax1.set_ylabel(\"Error percentage\")\n",
    "    ax2.set_ylabel(\"Training time\")\n",
    "    ax3.set_ylabel(\"Testing time\")\n",
    "\n",
    "    b1 = ax1.bar(x, err, bar_width, alpha=0.2, color='r')\n",
    "    b2 = ax2.bar(x + bar_width, time_train, bar_width, alpha=0.2, color='g')\n",
    "    b3 = ax3.bar(x + bar_width * 2, time_test, bar_width, alpha=0.2, color='b')\n",
    "    plt.xticks(x + bar_width * 2, names)\n",
    "    plt.legend([b1[0], b2[0], b3[0]], ('Error Percentage', 'Training Time', 'Testing Time'), loc='upper left')\n",
    "    plt.xlabel('Different Types Of Classifiers')\n",
    "    plt.title('Evaluation Of Different Classifiers')\n",
    "    plt.savefig(\"Performance_his.png\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "draw(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Running classifiers </h1>\n",
    "\n",
    "***\n",
    "Six classifiers were trained and tested. To enable running these trained classifiers on another email dataset, they were saved after being trained so that they can be loaded promptly later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving classifiers\n",
    "\n",
    "The **joblib** module in **sklearn.externals** comes handy for this purpose. Suppose we have a trained classifier **clf** and in order to save it, simply run **joblib.dump(clf, filename)**. The saving functionality was embeded within the method **test_clf** and after traning, six **joblib** files will be saved as\n",
    "\n",
    "> 1. MultinomialNB.joblib\n",
    "> 2. RandomForest.joblib\n",
    "> 3. Ridge_Regression.joblib\n",
    "> 4. BernoulliNB.joblib\n",
    "> 5. K_Neighbors.joblib\n",
    "> 6. SVC.joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(name, clf):\n",
    "    print (u'Classifier：', clf)\n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)\n",
    "    print (u'Training time for 5 -fold cross validation：%.3f/(5*%d) = %.3fsec' % ((t_end - t_start), m, t_train))\n",
    "    print( u'Optimal hyperparameter：', model.best_params_)\n",
    "    joblib.dump(model, \"%s.joblib\"%name)\n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test)\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start\n",
    "    print (u'Testing Time：%.3f sec' % t_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    print (u'Accuracy ：%.2f%%' % (100 * acc))\n",
    "    name = str(clf).split('(')[0]\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]     # 去掉末尾的Classifier\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "    return t_train, t_test, 1-acc, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving vectorizer \n",
    "\n",
    "In order to run classifiers on another dataset, the new dataset has to be preprocessed in exactly the same way of treating the original dataset. This requires to save the **TfidfVectorizer** after fitting it with **data_train**. New raw text will be transformed into **TF-IDF** vectors using this very vectorizer. In a similar way as saving the classifiers, the python in-built module **pickle** was utilized to save the **vectorizer**. The vectorizer was saved in the file ****vec.pickle****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(data_train, data_test):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)\n",
    "\n",
    "    vec = vectorizer.fit(data_train.data)\n",
    "    pickle.dump(vec, open(\"vec.pickle\", \"wb\"))\n",
    "    x_train = vectorizer.transform(data_train.data)  # x_train是稀疏的，scipy.sparse.csr.csr_matrix\n",
    "    x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "    return x_train, x_test, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running classifiers\n",
    "\n",
    "First specify the filename of the raw text and transform it into the **TF-IDF** vector using the method **get_tfidf**.\n",
    "\n",
    "### Loading the vectorizer\n",
    "\n",
    "The vectorized was fit and saved in the file **vec.pickle**. In order to load it, run **pickle.load(\"vec.pickle\")**. The raw text can be transformed into a **TF-IDF** vector using this vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-29e94bd6831c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-29e94bd6831c>\u001b[0m in \u001b[0;36mget_tfidf\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vec.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yanghaofan/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yanghaofan/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yanghaofan/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mproto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unsupported pickle protocol: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPROTO\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 3"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "run_file = \"test.txt\" # Please specify the file name you want to test\n",
    "\n",
    "def get_tfidf(filename):\n",
    "\ttext = \"\"\n",
    "\tvectorizer = pickle.load(open(\"vec.pickle\"))\n",
    "\twith open(filename) as fl:\n",
    "\t\tfor line in fl:\n",
    "\t\t\ttext += line\n",
    "\treturn vectorizer.transform([text]) \n",
    "\n",
    "x = get_tfidf(run_file)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the classifiers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
