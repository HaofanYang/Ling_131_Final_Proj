{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background ##\n",
    "  \n",
    "   Automatic emails classification can help us to distinguish and classify different category of emails, which filters the unsolicited emails, known as spam and increases people's working efficiency. The main goal of developing the classifiers algorithms is to identify spam or unwanted emails versus useful emails. Effective emails classification using machine learning algorithms can detect spam that contains lots of annoying advertisements and unneeded information and filter the spam in a high accuracy. Different classifications are trained to classify the spam email and wanted email behaves in a different result even using a same database. \n",
    "   \n",
    "   In order to enhance the effectiveness of email classification, the general content based spam filter are working based on words and phrases inside the email text. With certain keywords within the email, it would container as spam. The goal for this project is to finding the most effective algorithms. We apply five Machine Learning algorithms (as follows: Multinomial Naive Bayes, Bernoulli Naive Bayes, K Neighbors, Ridge Regression, Support Vector Machine and Random Forest) to the text and make an analysis to their performance based on email classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation ##\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "from sklearn.externals import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get Dataset\n",
    "We are going to use a built in method in **sklearn.dataset** to downloads data from 20newsgroups api with some chosen categories (computer science, science, electronics, sports), seperates training set and testing set, finally returns them.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start downloading...\n",
      "downloading completed，take 0.506 sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def get_data():\n",
    "    remove = ()\n",
    "    categories = 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'\n",
    "\n",
    "    print('start downloading...')\n",
    "    t_start = time()\n",
    "\n",
    "    data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "    data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "\n",
    "    t_end = time()\n",
    "    print('downloading completed，take %.3f sec' % (t_end - t_start))\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "data_train, data_test = get_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the dataset**\n",
    "\n",
    "Both *data_train* and *data_test* are of **Bunch** object, which has the following attributes\n",
    "\n",
    "> 1. bunch.data: list\n",
    "> 2. bunch.target: array\n",
    "> 3. bunch.filenames: list\n",
    "> 4. bunch.DESCR: a description of the dataset\n",
    "\n",
    "Let's take a look at the first raw text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: note to Bobby M.\n",
      "Lines: 52\n",
      "Organization: Walla Walla College\n",
      "Lines: 52\n",
      "\n",
      "In article <1993Apr14.190904.21222@daffy.cs.wisc.edu> mccullou@snake2.cs.wisc.edu (Mark McCullough) writes:\n",
      ">From: mccullou@snake2.cs.wisc.edu (Mark McCullough)\n",
      ">Subject: Re: note to Bobby M.\n",
      ">Date: Wed, 14 Apr 1993 19:09:04 GMT\n",
      ">In article <1993Apr14.131548.15938@monu6.cc.monash.edu.au> darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\n",
      ">>In <madhausC5CKIp.21H@netcom.co ...\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[0][:500], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type： <class 'sklearn.utils.Bunch'>\n",
      "# of texts in train set ： 2034\n",
      "# of texts in test set： 1353\n",
      "name of4 categories：\n",
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print('data type：', type(data_train))\n",
    "print('# of texts in train set ：', len(data_train.data))\n",
    "print('# of texts in test set：', len(data_test.data))\n",
    "print('name of%d categories：' % len(data_train.target_names))\n",
    "\n",
    "pprint(data_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Preprocessing data\n",
    "Now the dataset has been split into traning and test sets. In order to use those data to train classifiers, features and corresponding lables should be extrated via two methods **get_y_data()** and **tfidf_data()**\n",
    "\n",
    "> 1. **get_y_data(data_train, data_tetst)**: takes two datasets and returns an array of lables.\n",
    "> 2. **tfidf_data(data_train, data_test)**: takes two datasets and returns an array of tf-idf vectors.\n",
    "\n",
    "//TODO\n",
    "explain tfidf\n",
    "\n",
    "Additionally, in order to run classifiers on another dataset, the new dataset has to be preprocessed in exactly the same way of treating the original dataset. This requires to save the **TfidfVectorizer** after fitting it with data_train. New raw text will be transformed into **TF-IDF** vectors using this very vectorizer. In a similar way as saving the classifiers, the python in-built module **pickle** was utilized to save the vectorizer. The vectorizer was saved in the file **vec.pickle**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_data(data_train, data_test):\n",
    "\n",
    "    y_train = data_train.target\n",
    "    y_test = data_test.target\n",
    "\n",
    "    return y_train, y_test\n",
    "\n",
    "def tfidf_data(data_train, data_test):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)\n",
    "\n",
    "    vec = vectorizer.fit(data_train.data)\n",
    "    pickle.dump(vec, open(\"vec.pickle\", \"wb\"))\n",
    "    x_train = vectorizer.transform(data_train.data)  # x_train is sparse，scipy.sparse.csr.csr_matrix\n",
    "    x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "y_train, y_test = get_y_data(data_train, data_test)\n",
    "x_train, x_test = tfidf_data(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Examples : the first 2 texts -- \n",
      "----------------\n",
      "category for text1 : alt.atheism\n",
      "\n",
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: note to Bobby M.\n",
      "Lines: 52\n",
      "Organization: Walla Walla College\n",
      "Lines: 52\n",
      "\n",
      "In article <1993Apr14.190904.21222@daffy.cs.wisc.edu> mccullou@snake2.cs.wisc.edu (Mark McCullough) writes:\n",
      ">From: mccullou@snake2.cs.wisc.edu (Mark McCullough)\n",
      ">Subject: Re: note to Bobby M.\n",
      ">Date: Wed, 14 Apr 1993 19:09:04 GMT\n",
      ">In article <1993Apr14.131548.15938@monu6.cc.monash.edu.au> darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\n",
      ">>In <madhausC5CKIp.21H@netcom.co\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "category for text2 : comp.graphics\n",
      "\n",
      "From: ch381@cleveland.Freenet.Edu (James K. Black)\n",
      "Subject: NEEDED: algorithms for 2-d & 3-d object recognition\n",
      "Organization: Case Western Reserve University, Cleveland, OH (USA)\n",
      "Lines: 23\n",
      "Reply-To: ch381@cleveland.Freenet.Edu (James K. Black)\n",
      "NNTP-Posting-Host: hela.ins.cwru.edu\n",
      "\n",
      "\n",
      "Hi,\n",
      "         I have a friend who is working on 2-d and 3-d object recognition. He is looking\n",
      "for references describing algorithms on the following subject areas:\n",
      "\n",
      "Thresholding\n",
      "Edge Segmentation\n",
      "Marr-Hildreth\n",
      "Sobel Ope\n",
      "...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' -- Examples : the first 2 texts -- ')\n",
    "\n",
    "categories = data_train.target_names\n",
    "\n",
    "for i in np.arange(2):\n",
    "    print('----------------')\n",
    "    print('category for text%d : %s\\n' % (i + 1, categories[y_train[i]]))\n",
    "    print(data_train.data[i][:500])\n",
    "    print('...')\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train set：2034，# of features：33809\n"
     ]
    }
   ],
   "source": [
    "print('# of train set：%d，# of features：%d' % x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train Classifier ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method **classifier** uses 6 different classifiers (Multinomial Naive Bayes classifier, Bernoulli Naive Bayes classifier, K Neighbors classifier, Ridge Regression classifier, Random Forest classifier, Support Vector Machine classifier) to classify training data. \n",
    "\n",
    "It then returns array of results of each classifier including its error rate, training time and testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(x, y):\n",
    "    print('\\n\\n===================\\n evaluation of classifiers：\\n')\n",
    "    clfs = {\"MultinomialNB\": MultinomialNB(), \n",
    "            \"BernoulliNB\": BernoulliNB(),  \n",
    "            \"K_Neighbors\": KNeighborsClassifier(),  \n",
    "            \"Ridge_Regression\": RidgeClassifier(),  \n",
    "            \"RandomForest\": RandomForestClassifier(n_estimators=200),  \n",
    "            \"SVC\": SVC()  \n",
    "            }\n",
    "    result = []\n",
    "    for name,clf in clfs.items():\n",
    "        a = test_clf(name, clf, x, y)\n",
    "        result.append(a)\n",
    "        print('\\n')\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test_clf**'s input is one of the classifiers. GridSearchCV object's job is to set parameters to the classifier according to type of the classifier (e.g. set **neighbors_can** when the classifier is **K Neighbors classifier**). \n",
    "\n",
    "Then it trains each classifier.\n",
    "\n",
    "Six classifiers are going to be trained and tested. To enable running these trained classifiers on another email dataset, they should be saved after being trained so that they can be loaded promptly later.\n",
    "\n",
    "Regarding saving functionality, the **joblib** module in **sklearn.externals** comes handy for this purpose. Suppose we have a trained classifier **clf** and in order to save it, simply run **joblib.dump(clf, filename)**. The saving functionality was embeded within the method **test_clf** and after traning, six **joblib** files will be saved as\n",
    "\n",
    "> 1. MultinomialNB.joblib\n",
    "> 2. RandomForest.joblib\n",
    "> 3. Ridge_Regression.joblib\n",
    "> 4. BernoulliNB.joblib\n",
    "> 5. K_Neighbors.joblib\n",
    "> 6. SVC.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================\n",
      " evaluation of classifiers：\n",
      "\n",
      "Classifier： MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Training time for 5 -fold cross validation：0.351/(5*10) = 0.007sec\n",
      "Optimal hyperparameter： {'alpha': 0.003593813663804626}\n",
      "Testing Time：0.001 sec\n",
      "Accuracy ：89.58%\n",
      "\n",
      "\n",
      "Classifier： BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Training time for 5 -fold cross validation：0.492/(5*10) = 0.010sec\n",
      "Optimal hyperparameter： {'alpha': 0.001}\n",
      "Testing Time：0.003 sec\n",
      "Accuracy ：88.54%\n",
      "\n",
      "\n",
      "Classifier： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Training time for 5 -fold cross validation：11.115/(5*14) = 0.159sec\n",
      "Optimal hyperparameter： {'n_neighbors': 3}\n",
      "Testing Time：0.134 sec\n",
      "Accuracy ：86.03%\n",
      "\n",
      "\n",
      "Classifier： RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "Training time for 5 -fold cross validation：19.228/(5*10) = 0.385sec\n",
      "Optimal hyperparameter： {'alpha': 0.046415888336127795}\n",
      "Testing Time：0.001 sec\n",
      "Accuracy ：89.73%\n",
      "\n",
      "\n",
      "Classifier： RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Training time for 5 -fold cross validation：26.310/(5*6) = 0.877sec\n",
      "Optimal hyperparameter： {'max_depth': 9}\n",
      "Testing Time：0.120 sec\n",
      "Accuracy ：76.72%\n",
      "\n",
      "\n",
      "Classifier： SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "result = classifier(x_train, y_train)\n",
    "\n",
    "def test_clf(name, clf, x_train, y_train):\n",
    "    \n",
    "    print ('Classifier：', clf)\n",
    "    \n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    \n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    \n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)\n",
    "    \n",
    "    print ('Training time for 5 -fold cross validation：%.3f/(5*%d) = %.3fsec' % ((t_end - t_start), m, t_train))\n",
    "    print( 'Optimal hyperparameter：', model.best_params_)\n",
    "    \n",
    "    # save the classifier\n",
    "    joblib.dump(model, \"%s.joblib\"%name)\n",
    "    \n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test)\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start\n",
    "    print ('Testing Time：%.3f sec' % t_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    print ('Accuracy ：%.2f%%' % (100 * acc))\n",
    "    \n",
    "    name = str(clf).split('(')[0]\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "        \n",
    "    return t_train, t_test, 1-acc, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5) Render Results ####\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**draw** renders a diagram using results from previous method to evaluate results of different classifiers by listing the **error rate, training time and testing time** of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(result)\n",
    "\n",
    "def draw(result):\n",
    "    time_train1, time_test1, err1, names = result.T\n",
    "    time_test = time_test1.astype(np.float)\n",
    "    time_train = time_train1.astype(np.float)\n",
    "    err = err1.astype(np.float)\n",
    "    x= np.arange(len(time_train))\n",
    "    bar_width = 0.25\n",
    "    ax1 = host_subplot(111, axes_class=AA.Axes)\n",
    "    plt.subplots_adjust(right = 0.75)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    offset3 = 60\n",
    "    offset2 = 0\n",
    "\n",
    "    new_fixed_axis = ax3.get_grid_helper().new_fixed_axis\n",
    "    ax3.axis[\"right\"] = new_fixed_axis(loc=\"right\", axes=ax3, offset=(offset3, 0))\n",
    "    ax3.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    new_fixed_axis2 = ax2.get_grid_helper().new_fixed_axis\n",
    "    ax2.axis[\"right\"] = new_fixed_axis2(loc=\"right\", axes=ax2, offset=(offset2, 0))\n",
    "    ax2.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    ax1.set_ylabel(\"Error percentage\")\n",
    "    ax2.set_ylabel(\"Training time\")\n",
    "    ax3.set_ylabel(\"Testing time\")\n",
    "\n",
    "    b1 = ax1.bar(x, err, bar_width, alpha=0.2, color='r')\n",
    "    b2 = ax2.bar(x + bar_width, time_train, bar_width, alpha=0.2, color='g')\n",
    "    b3 = ax3.bar(x + bar_width * 2, time_test, bar_width, alpha=0.2, color='b')\n",
    "    plt.xticks(x + bar_width * 2, names)\n",
    "    plt.legend([b1[0], b2[0], b3[0]], ('Error Percentage', 'Training Time', 'Testing Time'), loc='upper left')\n",
    "    plt.xlabel('Different Types Of Classifiers')\n",
    "    plt.title('Evaluation Of Different Classifiers')\n",
    "    plt.savefig(\"Performance_his.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running classifiers\n",
    "***\n",
    "\n",
    "First specify the filename of the raw text and transform it into the **TF-IDF** vector using the method **get_tfidf**.\n",
    "\n",
    "#### 2.1 Loading the vectorizer\n",
    "\n",
    "The vectorized was fit and saved in the file **vec.pickle**. In order to load it, run **pickle.load(\"vec.pickle\")**. The raw text can be transformed into a **TF-IDF** vector using this vectorizer.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "run_file = \"test.txt\" # Please specify the file name you want to test\n",
    "\n",
    "def get_tfidf(filename):\n",
    "\ttext = \"\"\n",
    "\tvectorizer = pickle.load(open(\"vec.pickle\"))\n",
    "\twith open(filename) as fl:\n",
    "\t\tfor line in fl:\n",
    "\t\t\ttext += line\n",
    "\treturn vectorizer.transform([text]) \n",
    "\n",
    "x = get_tfidf(run_file)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Loading classifiers\n",
    "\n",
    "The raw text has been transformed into the **TF-IDF** vector and it can be classified using saved classifiers. Define a method **run(x, clf_name)** where x is a **TF-IDF** vector and **clf_name** is the filename for a classifier. It returns the y value for the given x\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x, clf_name):\n",
    "\tclf_name = clf_name + \".joblib\"\n",
    "\tclf = joblib.load(clf_name)\n",
    "\treturn clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The classification for %s is: \\n\" % run_file)\n",
    "clfs = (\"MultinomialNB\", \"BernoulliNB\",  \"K_Neighbors\",  \"Ridge_Regression\",  \"RandomForest\",  \"SVC\")\n",
    "cat = ('alt.atheism', 'comp.graphics','sci.space', 'talk.religion.misc')\n",
    "for clf in clfs:\n",
    "    print(\"%s: %s\\n\" % (clf, cat[run(x, clf)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
